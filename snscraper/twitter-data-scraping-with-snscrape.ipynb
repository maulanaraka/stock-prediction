{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905eb33e",
   "metadata": {
    "papermill": {
     "duration": 0.006083,
     "end_time": "2023-05-27T12:07:25.095087",
     "exception": false,
     "start_time": "2023-05-27T12:07:25.089004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SNSCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead7ca74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:07:25.107519Z",
     "iopub.status.busy": "2023-05-27T12:07:25.107149Z",
     "iopub.status.idle": "2023-05-27T12:07:25.112435Z",
     "shell.execute_reply": "2023-05-27T12:07:25.111277Z"
    },
    "papermill": {
     "duration": 0.016822,
     "end_time": "2023-05-27T12:07:25.117165",
     "exception": false,
     "start_time": "2023-05-27T12:07:25.100343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip3 install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cad8529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:07:25.129173Z",
     "iopub.status.busy": "2023-05-27T12:07:25.128766Z",
     "iopub.status.idle": "2023-05-27T12:07:26.214442Z",
     "shell.execute_reply": "2023-05-27T12:07:26.213112Z"
    },
    "papermill": {
     "duration": 1.094988,
     "end_time": "2023-05-27T12:07:26.217225",
     "exception": false,
     "start_time": "2023-05-27T12:07:25.122237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.3\n"
     ]
    }
   ],
   "source": [
    "#Supported only by versions higher than 3.8\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc066dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:07:26.229547Z",
     "iopub.status.busy": "2023-05-27T12:07:26.229159Z",
     "iopub.status.idle": "2023-05-27T12:08:09.338388Z",
     "shell.execute_reply": "2023-05-27T12:08:09.337091Z"
    },
    "papermill": {
     "duration": 43.11896,
     "end_time": "2023-05-27T12:08:09.341365",
     "exception": false,
     "start_time": "2023-05-27T12:07:26.222405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\maula\\appdata\\local\\temp\\pip-req-build-87g9o6z6\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 614d4c2029a62d348ca56598f87c425966aaec66\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests[socks] in d:\\data\\document\\.portfolio project\\stock-prediction\\.venv\\lib\\site-packages (from snscrape==0.7.0.20230622) (2.32.4)\n",
      "Collecting lxml (from snscrape==0.7.0.20230622)\n",
      "  Downloading lxml-6.0.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting beautifulsoup4 (from snscrape==0.7.0.20230622)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from snscrape==0.7.0.20230622)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->snscrape==0.7.0.20230622)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\data\\document\\.portfolio project\\stock-prediction\\.venv\\lib\\site-packages (from beautifulsoup4->snscrape==0.7.0.20230622) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\data\\document\\.portfolio project\\stock-prediction\\.venv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\data\\document\\.portfolio project\\stock-prediction\\.venv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\data\\document\\.portfolio project\\stock-prediction\\.venv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\data\\document\\.portfolio project\\stock-prediction\\.venv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (2025.8.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\data\\document\\.portfolio project\\stock-prediction\\.venv\\lib\\site-packages (from requests[socks]->snscrape==0.7.0.20230622) (1.7.1)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading lxml-6.0.0-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.6/4.0 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.9/4.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 5.8 MB/s  0:00:00\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (pyproject.toml): started\n",
      "  Building wheel for snscrape (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for snscrape: filename=snscrape-0.7.0.20230622-py3-none-any.whl size=75387 sha256=a0ec7f9f0a30fd19ad92e3dc1e42ea7365107e9c1f982542fd1450a2d0fe2696\n",
      "  Stored in directory: C:\\Users\\maula\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ig23zzm2\\wheels\\9d\\c8\\17\\cef67c1f1958893b354f7fa913774d382e0abfd371e1b70095\n",
      "Successfully built snscrape\n",
      "Installing collected packages: soupsieve, lxml, filelock, beautifulsoup4, snscrape\n",
      "\n",
      "   ---------------------------------------- 0/5 [soupsieve]\n",
      "   -------- ------------------------------- 1/5 [lxml]\n",
      "   -------- ------------------------------- 1/5 [lxml]\n",
      "   -------- ------------------------------- 1/5 [lxml]\n",
      "   -------- ------------------------------- 1/5 [lxml]\n",
      "   ---------------- ----------------------- 2/5 [filelock]\n",
      "   ------------------------ --------------- 3/5 [beautifulsoup4]\n",
      "   ------------------------ --------------- 3/5 [beautifulsoup4]\n",
      "   ------------------------ --------------- 3/5 [beautifulsoup4]\n",
      "   ------------------------ --------------- 3/5 [beautifulsoup4]\n",
      "   ------------------------ --------------- 3/5 [beautifulsoup4]\n",
      "   -------------------------------- ------- 4/5 [snscrape]\n",
      "   -------------------------------- ------- 4/5 [snscrape]\n",
      "   ---------------------------------------- 5/5 [snscrape]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 filelock-3.18.0 lxml-6.0.0 snscrape-0.7.0.20230622 soupsieve-2.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\maula\\AppData\\Local\\Temp\\pip-req-build-87g9o6z6'\n"
     ]
    }
   ],
   "source": [
    "#Developer version\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154ae5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.359312Z",
     "iopub.status.busy": "2023-05-27T12:08:09.358920Z",
     "iopub.status.idle": "2023-05-27T12:08:09.680148Z",
     "shell.execute_reply": "2023-05-27T12:08:09.679051Z"
    },
    "papermill": {
     "duration": 0.333742,
     "end_time": "2023-05-27T12:08:09.682885",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.349143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing the necessary modules\n",
    "import snscrape.modules.twitter as twitterscraper\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc79d22",
   "metadata": {
    "papermill": {
     "duration": 0.007177,
     "end_time": "2023-05-27T12:08:09.697574",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.690397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Twitterscraper**\n",
    "\n",
    "There are different methods that can be used to scrape tweets and extract information from Twitter using the snscrape library.\n",
    "\n",
    "TwitterUserScraper: Scrape tweets from a user's timeline.\n",
    "TwitterSearchScraper: Scrape tweets containing specific keywords.\n",
    "TwitterHashtagScraper: Scrape tweets from a specific hashtag.\n",
    "TwitterGeoSearchScraper: Scrape tweets from a specific location based on geolocation.\n",
    "\n",
    "'TwitterHashtagScraper' is used below as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043045b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape [-h] [--version] [--citation] [-v] [--dump-locals] [--retry N]\n",
      "                [-n N] [-f FORMAT | --jsonl | --jsonl-for-buggy-int-parser]\n",
      "                [--with-entity] [--since DATETIME] [--progress]\n",
      "                SCRAPER ...\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show program's version number and exit\n",
      "  --citation            Display recommended citation information and exit\n",
      "                        (default: None)\n",
      "  -v, --verbose, --verbosity\n",
      "                        Increase output verbosity (default: 0)\n",
      "  --dump-locals         Dump local variables on serious log messages (warnings\n",
      "                        or higher) (default: False)\n",
      "  --retry N, --retries N\n",
      "                        When the connection fails or the server returns an\n",
      "                        unexpected response, retry up to N times with an\n",
      "                        exponential backoff (default: 3)\n",
      "  -n N, --max-results N\n",
      "                        Only return the first N results (default: None)\n",
      "  -f FORMAT, --format FORMAT\n",
      "                        Output format (default: None)\n",
      "  --jsonl               Output JSONL (default: False)\n",
      "  --jsonl-for-buggy-int-parser\n",
      "                        Output JSONL and insert extra string fields into\n",
      "                        objects for integers exceeding double precision limits\n",
      "                        (default: False)\n",
      "  --with-entity         Include the entity (e.g. user, channel) as the first\n",
      "                        output item (default: False)\n",
      "  --since DATETIME      Only return results newer than DATETIME (default:\n",
      "                        None)\n",
      "  --progress            Report progress on stderr (default: False)\n",
      "\n",
      "scrapers:\n",
      "  SCRAPER\n",
      "    facebook-community\n",
      "    facebook-group\n",
      "    facebook-user\n",
      "    instagram-hashtag\n",
      "    instagram-location\n",
      "    instagram-user\n",
      "    mastodon-profile\n",
      "    mastodon-toot\n",
      "    reddit-search\n",
      "    reddit-submission\n",
      "    reddit-subreddit\n",
      "    reddit-user\n",
      "    telegram-channel\n",
      "    twitter-cashtag\n",
      "    twitter-community\n",
      "    twitter-hashtag\n",
      "    twitter-list-posts\n",
      "    twitter-profile\n",
      "    twitter-search\n",
      "    twitter-trends\n",
      "    twitter-tweet\n",
      "    twitter-user\n",
      "    twitter-users\n",
      "    vkontakte-user\n",
      "    weibo-user\n"
     ]
    }
   ],
   "source": [
    "!snscrape --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3fbe505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 17:19:44.706  ERROR  snscrape.base  Error retrieving https://twitter.com/search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\"))\n",
      "2025-08-06 17:19:44.706  CRITICAL  snscrape.base  4 requests to https://twitter.com/search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click failed, giving up.\n",
      "2025-08-06 17:19:44.706  CRITICAL  snscrape.base  Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\"))\n",
      "2025-08-06 17:19:44.762  CRITICAL  snscrape._cli  Dumped stack and locals to C:\\Users\\maula\\AppData\\Local\\Temp\\snscrape_locals_f1kgcmfd\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Scripts\\snscrape.exe\\__main__.py\", line 6, in <module>\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\_cli.py\", line 323, in main\n",
      "    for i, item in enumerate(scraper.get_items(), start = 1):\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py\", line 1763, in get_items\n",
      "    for obj in self._iter_api_data('https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline', _TwitterAPIType.GRAPHQL, params, paginationParams, cursor = self._cursor, instructionsPath = ['data', 'search_by_raw_query', 'search_timeline', 'timeline', 'instructions']):\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py\", line 915, in _iter_api_data\n",
      "    obj = self._get_api_data(endpoint, apiType, reqParams, instructionsPath = instructionsPath)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py\", line 883, in _get_api_data\n",
      "    self._ensure_guest_token()\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py\", line 825, in _ensure_guest_token\n",
      "    r = self._get(self._baseUrl if url is None else url, responseOkCallback = self._check_guest_token_response)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\base.py\", line 275, in _get\n",
      "    return self._request('GET', *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\base.py\", line 271, in _request\n",
      "    raise ScraperException(msg)\n",
      "snscrape.base.ScraperException: 4 requests to https://twitter.com/search?f=live&lang=en&q=%23saham&src=spelling_expansion_revert_click failed, giving up.\n"
     ]
    }
   ],
   "source": [
    "!snscrape --max-results 100 twitter-hashtag saham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48fe65f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.714723Z",
     "iopub.status.busy": "2023-05-27T12:08:09.713506Z",
     "iopub.status.idle": "2023-05-27T12:08:09.720543Z",
     "shell.execute_reply": "2023-05-27T12:08:09.719518Z"
    },
    "papermill": {
     "duration": 0.017869,
     "end_time": "2023-05-27T12:08:09.722800",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.704931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraper=twitterscraper.TwitterHashtagScraper(\"#finance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dd00728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.741149Z",
     "iopub.status.busy": "2023-05-27T12:08:09.740226Z",
     "iopub.status.idle": "2023-05-27T12:08:09.744615Z",
     "shell.execute_reply": "2023-05-27T12:08:09.743901Z"
    },
    "papermill": {
     "duration": 0.016306,
     "end_time": "2023-05-27T12:08:09.746634",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.730328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets=scraper.get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34c3b6ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.763391Z",
     "iopub.status.busy": "2023-05-27T12:08:09.762716Z",
     "iopub.status.idle": "2023-05-27T12:08:09.766555Z",
     "shell.execute_reply": "2023-05-27T12:08:09.765839Z"
    },
    "papermill": {
     "duration": 0.01475,
     "end_time": "2023-05-27T12:08:09.768797",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.754047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweetsf=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34dd2cf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:09.785304Z",
     "iopub.status.busy": "2023-05-27T12:08:09.784944Z",
     "iopub.status.idle": "2023-05-27T12:08:47.687007Z",
     "shell.execute_reply": "2023-05-27T12:08:47.685965Z"
    },
    "papermill": {
     "duration": 37.913575,
     "end_time": "2023-05-27T12:08:47.689731",
     "exception": false,
     "start_time": "2023-05-27T12:08:09.776156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://twitter.com/search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\"))\n",
      "4 requests to https://twitter.com/search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click failed, giving up.\n",
      "Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)')))\"))\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://twitter.com/search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScraperException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtweets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m>\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:1763\u001b[39m, in \u001b[36mTwitterSearchScraper.get_items\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1760\u001b[39m params = {\u001b[33m'\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m'\u001b[39m: variables, \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: features}\n\u001b[32m   1761\u001b[39m paginationParams = {\u001b[33m'\u001b[39m\u001b[33mvariables\u001b[39m\u001b[33m'\u001b[39m: paginationVariables, \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: features}\n\u001b[32m-> \u001b[39m\u001b[32m1763\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_TwitterAPIType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGRAPHQL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaginationParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_by_raw_query\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_timeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1764\u001b[39m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graphql_timeline_instructions_to_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_by_raw_query\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearch_timeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimeline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:915\u001b[39m, in \u001b[36m_TwitterAPIScraper._iter_api_data\u001b[39m\u001b[34m(self, endpoint, apiType, params, paginationParams, cursor, direction, instructionsPath)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    914\u001b[39m \t_logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m \tobj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapiType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructionsPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[32m    918\u001b[39m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:883\u001b[39m, in \u001b[36m_TwitterAPIScraper._get_api_data\u001b[39m\u001b[34m(self, endpoint, apiType, params, instructionsPath)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_api_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint, apiType, params, instructionsPath = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m \t\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_guest_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    884\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m apiType \u001b[38;5;129;01mis\u001b[39;00m _TwitterAPIType.GRAPHQL:\n\u001b[32m    885\u001b[39m \t\tparams = urllib.parse.urlencode({k: json.dumps(v, separators = (\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.items()}, quote_via = urllib.parse.quote)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\modules\\twitter.py:825\u001b[39m, in \u001b[36m_TwitterAPIScraper._ensure_guest_token\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._guestTokenManager.token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    824\u001b[39m \t_logger.info(\u001b[33m'\u001b[39m\u001b[33mRetrieving guest token\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \tr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_baseUrl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_guest_token_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m (match := re.search(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.cookie = decodeURIComponent\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgt=(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+); Max-Age=10800; Domain=\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.twitter\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.com; Path=/; Secure\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m);\u001b[39m\u001b[33m'\u001b[39m, r.text)):\n\u001b[32m    827\u001b[39m \t\t_logger.debug(\u001b[33m'\u001b[39m\u001b[33mFound guest token in HTML\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\base.py:275\u001b[39m, in \u001b[36mScraper._get\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Data\\Document\\.Portfolio Project\\stock-prediction\\.venv\\Lib\\site-packages\\snscrape\\base.py:271\u001b[39m, in \u001b[36mScraper._request\u001b[39m\u001b[34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[39m\n\u001b[32m    269\u001b[39m \t_logger.fatal(msg)\n\u001b[32m    270\u001b[39m \t_logger.fatal(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mReached unreachable code\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mScraperException\u001b[39m: 4 requests to https://twitter.com/search?f=live&lang=en&q=%23%23finance&src=spelling_expansion_revert_click failed, giving up."
     ]
    }
   ],
   "source": [
    "for i,tweet in enumerate(tweets):\n",
    "    if i>100:\n",
    "        break\n",
    "    tweetsf.append(tweet.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18661d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:47.706860Z",
     "iopub.status.busy": "2023-05-27T12:08:47.706400Z",
     "iopub.status.idle": "2023-05-27T12:08:47.713166Z",
     "shell.execute_reply": "2023-05-27T12:08:47.712191Z"
    },
    "papermill": {
     "duration": 0.018025,
     "end_time": "2023-05-27T12:08:47.715432",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.697407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(tweetsf)):\n",
    "    tweetsf[i]=tweetsf[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aab396f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:47.732072Z",
     "iopub.status.busy": "2023-05-27T12:08:47.731640Z",
     "iopub.status.idle": "2023-05-27T12:08:47.743543Z",
     "shell.execute_reply": "2023-05-27T12:08:47.742467Z"
    },
    "papermill": {
     "duration": 0.022717,
     "end_time": "2023-05-27T12:08:47.745720",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.723003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Tweet':tweetsf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ead7d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T12:08:47.762848Z",
     "iopub.status.busy": "2023-05-27T12:08:47.762060Z",
     "iopub.status.idle": "2023-05-27T12:08:47.792022Z",
     "shell.execute_reply": "2023-05-27T12:08:47.790641Z"
    },
    "papermill": {
     "duration": 0.041301,
     "end_time": "2023-05-27T12:08:47.794475",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.753174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Tweet]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd0e71",
   "metadata": {
    "papermill": {
     "duration": 0.007453,
     "end_time": "2023-05-27T12:08:47.810498",
     "exception": false,
     "start_time": "2023-05-27T12:08:47.803045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "The snscrape library provides support for scraping data from various social media platforms in addition to Twitter. Here are some of the other platforms that you can scrape using snscrape:\n",
    "\n",
    "**Reddit**: You can scrape data from Reddit, including posts and comments, using the *snscrape.modules.reddit* module.\n",
    "\n",
    "**Instagram**: snscrape allows you to scrape data from Instagram, such as posts and comments. However, please note that Instagram's terms of service prohibit automated scraping, so make sure to comply with their policies.\n",
    "\n",
    "**YouTube**: You can scrape data from YouTube, including video metadata and comments, using the *snscrape.modules.youtube* module.\n",
    "\n",
    "**TikTok**: snscrape supports scraping data from TikTok, such as user profiles and videos, using the *snscrape.modules.tiktok* module.\n",
    "\n",
    "**Weibo**: snscrape enables scraping data from Weibo, a popular Chinese social media platform, using the *snscrape.modules.weibo* module.\n",
    "\n",
    "**Pinterest**: snscrape provides support for scraping data from Pinterest, including pins, boards, and user profiles, using the *snscrape.modules.pinterest* module.\n",
    "\n",
    "It's recommended to refer to the snscrape documentation for more details on each specific platform: https://github.com/JustAnotherArchivist/snscrape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 95.607587,
   "end_time": "2023-05-27T12:08:48.740864",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-27T12:07:13.133277",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
